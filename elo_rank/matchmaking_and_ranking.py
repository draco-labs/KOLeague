# -*- coding: utf-8 -*-
"""
Created on Tue Jan 14 14:23:03 2025

@author: tangu
"""

import pandas as pd
from tqdm import tqdm
import json
import plotly.graph_objects as go
from datetime import datetime, timedelta
from pymongo import MongoClient
import matplotlib.pyplot as plt
import numpy as np
from pandas import Timestamp




def matchmaking(filtered_tweets, elo_df, match_df, match_id_initiate):
    """
    Perform matchmaking and ELO calculation for filtered tweets, updating player scores after each match.

    Args:
        filtered_tweets (DataFrame): The tweets data grouped by coin and date.
        elo_df (DataFrame): The current ELO ratings for all players.
        match_df (DataFrame): The dataframe to store match results.
        match_id_initiate (int): Initial match ID to start from.

    Returns:
        DataFrame: Updated match results with ELO calculations and player scores.
    """
    # Initialize match count
    match_count = match_id_initiate

    # Group tweets by check_time and coin
    grouped = filtered_tweets.groupby(['check_time', 'coin'])

    for (date, coin), group in tqdm(grouped):
        #if match_count>100: break #debug
        #(f'\nDate: {date}, Coin: {coin} __________________________________')

        # Copy the group for processing
        player_df = group.copy()

        # Merge ELO data and rename columns
        player_df = player_df.merge(elo_df, on='screen_name')
        player_df = player_df.sort_values('elo').rename(columns={'elo': 'elo_before'})

        # Skip if there are less than 2 players
        if len(group) < 2:
            continue

        # Split into buy and sell groups
        buy_group = player_df[player_df['action_prompt'] == 'buy']
        sell_group = player_df[player_df['action_prompt'] == 'sell']

        # Sort groups by ELO before
        buy_group = buy_group.sort_values('elo_before', ascending=False)
        sell_group = sell_group.sort_values('elo_before', ascending=False)

        # Add placeholders to balance groups
        difference = abs(len(buy_group) - len(sell_group))
        if len(buy_group) < len(sell_group):
            latest_time = sell_group['tweet_time'].max()
            latest_price = sell_group['price_at_call'].iloc[-1]
            latest_price_check = sell_group['price_at_check'].iloc[-1]
            placeholders = [{
                'action_prompt': 'buy',
                'detected_coins': sell_group['detected_coins'].iloc[0],
                'full_text': 'tweet generated by BOT',
                'post_url': 'NA',
                'screen_name': 'BOT',
                'term_classification': 'NA',
                '_id': 0,
                'name': 'BOT',
                'avatar': 'NA',
                'tweet_time': latest_time,
                'price_at_call': latest_price,
                'check_time': sell_group['check_time'].iloc[-1],
                'price_at_check': latest_price_check,
                'ROI': -sell_group['ROI'].iloc[-1],
                'coin': sell_group['coin'].iloc[0],
                'tweet_date': sell_group['tweet_date'].iloc[-1],
                'elo_before': 1000
            } for _ in range(difference)]
            buy_group = pd.concat([buy_group, pd.DataFrame(placeholders)], ignore_index=True)
        elif len(sell_group) < len(buy_group):
            latest_time = buy_group['tweet_time'].max()
            latest_price = buy_group['price_at_call'].iloc[-1]
            latest_price_check = buy_group['price_at_check'].iloc[-1]
            placeholders = [{
                'action_prompt': 'sell',
                'detected_coins': buy_group['detected_coins'].iloc[0],
                'full_text': 'tweet generated by BOT',
                'post_url': 'NA',
                'screen_name': 'BOT',
                'term_classification': 'NA',
                '_id': 0,
                'name': 'BOT',
                'avatar': 'NA',
                'tweet_time': latest_time,
                'price_at_call': latest_price,
                'check_time': buy_group['check_time'].iloc[-1],
                'price_at_check': latest_price_check,
                'ROI': -buy_group['ROI'].iloc[-1],
                'coin': buy_group['coin'].iloc[0],
                'tweet_date': buy_group['tweet_date'].iloc[-1],
                'elo_before': 1000
            } for _ in range(difference)]
            sell_group = pd.concat([sell_group, pd.DataFrame(placeholders)], ignore_index=True)



        # Add tags to both groups
        buy_group = buy_group.sort_values(by='elo_before', na_position='last').reset_index(drop=True)
        sell_group = sell_group.sort_values(by='elo_before', na_position='last').reset_index(drop=True)
        buy_group['tag'] = range(1, len(buy_group) + 1)
        sell_group['tag'] = range(1, len(sell_group) + 1)

        queue_df = pd.DataFrame()


        # Matchmaking based on tags
        for tag in range(1, max(len(buy_group), len(sell_group)) + 1):
            buy_row = buy_group[buy_group['tag'] == tag]
            sell_row = sell_group[sell_group['tag'] == tag]

            if not buy_row.empty and not sell_row.empty:
                queue_df = pd.concat([buy_row, sell_row], ignore_index=True)
                queue_df = queue_df.sort_values('ROI', ascending=False).reset_index(drop=True)
                queue_df['position'] = queue_df.index + 1

                queue_df.loc[queue_df['position'] == 1, 'game_result'] = 'w'
                queue_df.loc[queue_df['position'] != 1, 'game_result'] = 'l'

                present_win_elo = queue_df[queue_df['game_result'] == 'w']['elo_before'].min()
                present_lose_elo = queue_df[queue_df['game_result'] == 'l']['elo_before'].max()

                K = 32
                elo_after_list = []

                for _, row in queue_df.iterrows():
                    elo_before = row['elo_before']
                    if row['game_result'] == 'w':
                        expected_score = 1 / (1 + 10 ** ((present_lose_elo - elo_before) / 400))
                        elo_after = elo_before + K * (1 - expected_score) if row['screen_name'] != 'BOT' else elo_before
                    else:
                        expected_score = 1 / (1 + 10 ** ((present_win_elo - elo_before) / 400))
                        elo_after = elo_before + K * (0 - expected_score) if row['screen_name'] != 'BOT' else elo_before
                    elo_after_list.append(elo_after)

                queue_df['elo_after'] = elo_after_list
                queue_df['elo_change'] = queue_df['elo_after'] - queue_df['elo_before']
                queue_df['match_id'] = match_count

                #match_df = pd.concat([match_df, queue_df], ignore_index=True)

                # Update ELO scores for non-BOT players
                elo_update = queue_df[queue_df['screen_name'] != 'BOT'][['screen_name', 'elo_change']]
                elo_df = elo_df.merge(elo_update, on='screen_name', how='left')
                elo_df['elo_change'] = elo_df['elo_change'].fillna(0)
                elo_df['elo'] = elo_df['elo'] + elo_df['elo_change']
                elo_df = elo_df.drop(['elo_change'], axis=1)

                match_count += 1
        print('------------------queue_df\n',queue_df)
        match_df=pd.concat([match_df,queue_df])
    # Finalize match_df
    
    match_df['ghost_match']=False # place holder for the old algo
    
    
    match_df = match_df.sort_values(by=['match_id','position'])
    
    moving_list=['match_id','tweet_date','coin','screen_name','name','ghost_match','ROI','game_result','position','elo_before','elo_after','elo_change','tweet_time','price_at_call', 'check_time','price_at_check']
    
#    match_df=match_df.drop('detected_coins',axis=1)
    
    match_df = match_df[moving_list + [col for col in match_df.columns if col not in moving_list]]
    match_df['tweet_date'] = pd.to_datetime(match_df['tweet_date'], errors='coerce').dt.date

    #match_df= match_df.dropna(subset=["ROI"])
    match_df.to_csv('match.csv',index=None)
    
    
    return match_df


def finalizing(match_df):

    match_df_origin = match_df.copy()  # Keep the original for future use
    match_df=match_df[match_df['ghost_match']==False]
    
    # First, calculate the win rate for each screen_name
    win_rate_df = match_df.groupby('screen_name').apply(
        lambda x: (x['game_result'] == 'w').mean() * 100  # Calculate win rate as percentage
    ).reset_index(name='win_rate')
    
    # Calculate the total buy and sell actions for each screen_name
    action_counts_df = match_df.groupby('screen_name').apply(
        lambda x: pd.Series({
            'total_buy_calls': (x['action_prompt'] == 'buy').sum(),
            'total_sell_calls': (x['action_prompt'] == 'sell').sum()
        })
    ).reset_index()
        
    
    # remove ghost match
    
    # Calculate the total number of matches per screen_name
    total_matches_df = match_df.groupby('screen_name')['match_id'].nunique().reset_index(name='total_matches')
    
    # Get the latest ELO rating for each player
    latest_elo_df = match_df.groupby('screen_name').last()[['elo_after', 'avatar', 'name']].reset_index()
    latest_elo_df = latest_elo_df.rename(columns={'elo_after': 'latest_elo'})
    
    print('latest_elo_df',latest_elo_df)
    
    # Merge all these into a single final table
    final_table = latest_elo_df.merge(win_rate_df, on='screen_name').merge(total_matches_df, on='screen_name').merge(action_counts_df, on='screen_name')
    
    final_table = final_table.sort_values(by='latest_elo', ascending=False).reset_index(drop=True)
    final_table['rank'] = final_table.index + 1
    
    final_table_origin=final_table.copy()
    
    
    def assign_tier(rank,df):
        if rank <= len(df) * 0.001:
            return 'Grandmaster'
        elif rank <= len(df) * 0.005:
            return 'Master'
        elif rank <= len(df) * 0.05:
            return 'Diamond'
        elif rank <= len(df) * 0.1:
            return 'Platinum'
        elif rank <= len(df) * 0.25:
            return 'Gold'
        elif rank <= len(df) * 0.5:
            return 'Silver'
        else:
            return 'Bronze'
    
    # Apply the function to create the `tier` column
    final_table['tier'] = final_table['rank'].apply(lambda rank: assign_tier(rank, final_table))
    
    
    # Reorder columns to place 'rank' and 'tier' first
    final_table = final_table[['rank',
                               'tier',
                               'screen_name',
                               'name',
                               'avatar',
                               'latest_elo',
                               'win_rate',
                               'total_matches',
                               'total_buy_calls',
                               'total_sell_calls']]
    
    
    coin_usage = match_df.groupby(['screen_name', 'coin']).size().reset_index(name='coin_usage')

    # Sort by usage (descending) and get the top 5 coins for each player
    top_coins = coin_usage.groupby('screen_name').apply(
        lambda x: ';'.join(x.sort_values(by='coin_usage', ascending=False).head(5)['coin'])
    ).reset_index(name='top_coins')


    # Calculate the average ROI for each screen_name
    average_roi_df = match_df.groupby('screen_name')['ROI'].mean().reset_index(name='average_ROI')

    # Merge the average ROI into the final summary table
    final_table = final_table.merge(average_roi_df, on='screen_name', how='left')


    # Merge the top 5 coins information into the final summary table
    final_table = final_table.merge(top_coins, on='screen_name', how='left')


    match_df = match_df_origin
    
    # Round numerical columns to 2 decimal places
    match_df['ROI'] = match_df['ROI'].round(2)
    match_df['elo_before'] = match_df['elo_before'].round(2)
    match_df['elo_after'] = match_df['elo_after'].round(2)
    match_df['elo_change'] = match_df['elo_change'].round(2)
    
    final_table['latest_elo'] = final_table['latest_elo'].round(2)
    final_table['win_rate'] = final_table['win_rate'].round(2)
    final_table['average_ROI'] = final_table['average_ROI'].round(2)
    final_table['link'] = 'https://x.com/' + final_table['screen_name']
    
    # Drop 'Unnamed: 0' column if it exists
    if 'Unnamed: 0' in match_df.columns:
        match_df = match_df.drop(columns=['Unnamed: 0'])
    
    if 'Unnamed: 0' in final_table.columns:
        final_table = final_table.drop(columns=['Unnamed: 0'])
    
    print('final table',final_table)
    
    final_table.to_csv('final_table.csv')
    final_table.to_json('leader_board.json', orient='records', indent=4)
    return final_table,match_df_origin



def match_history_to_json(match_df_origin, coin_dict):
    print('match_df', match_df_origin)
    
    mdf = match_df_origin
    try:
        mdf = mdf.drop(columns=['Unnamed: 0'])
    except:
        pass
    mdf[['post_url', 'avatar']] = mdf[['post_url', 'avatar']].replace(0, "NA")  # fix for string type
    
    # Fill NaN values with zero
    mdf = mdf.fillna(0)
    mdf['match_time'] = mdf.index
    
    # Merge to get opponent's information by matching on 'match_id' and excluding the same screen_name
    merged_mdf = mdf.merge(
        mdf[['match_id', 'screen_name', 'avatar', 'name', 'ROI', 'tweet_time', 'full_text', 'post_url', 'action_prompt']],
        on='match_id',
        suffixes=('', '_opponent')
    )
    
    # Filter out rows where screen_name matches screen_name_opponent (to exclude self-match)
    merged_mdf = merged_mdf[merged_mdf['screen_name'] != merged_mdf['screen_name_opponent']]
    
    # Exclude matches where the player's ghost_match is True, but keep opponent's ghost matches
    detailed_mdf = merged_mdf[merged_mdf['ghost_match'] == False]
    
    # Group by screen_name, sort by tweet_date, take the last 10, add IDs, remove screen_name columns
    def assign_ids(group):
        group = group.sort_values(by='match_time', ascending=False).head(10)
        group['id'] = range(1, len(group) + 1)
        columns = ['id'] + [col for col in group.columns if col != 'id']
        return group[columns]
    
    detailed_mdf = detailed_mdf.groupby('screen_name', group_keys=False).apply(assign_ids)
    
    # Define the time intervals you want to calculate
    time_intervals = {
        '15m': 15,
        '1h': 60,
        '3h': 180,
        '4h': 240,
        '24h': 1440
    }
    
    price_record_tracks = {interval: [] for interval in time_intervals}
    
    for index, row in tqdm(detailed_mdf.iterrows(), desc='Adding price tracks'):
        coin = row['coin']
        tweet_time = pd.to_datetime(row['tweet_time'])
        
        for interval, minutes in time_intervals.items():
            # Calculate the time range for the current interval
            start_time = tweet_time - timedelta(minutes=minutes * 10)  # 10 bars before
            end_time = tweet_time + timedelta(minutes=minutes * 10)  # 10 bars after
        
            # Resample the data to the specified time interval
            coin_df = coin_dict[coin]
            
            # fix time-format bug 1--
            if 'date' in coin_df.columns:
                coin_df['timestamp'] = pd.to_datetime(coin_df['date'])
                
            # Check if the column is already timezone-aware
            if coin_df['timestamp'].dt.tz is None:
                coin_df['timestamp'] = coin_df['timestamp'].dt.tz_localize('UTC')
            else:
                coin_df['timestamp'] = coin_df['timestamp'].dt.tz_convert('UTC')    

            tweet_time = pd.to_datetime(tweet_time)
            if tweet_time.tzinfo is None:
                tweet_time = tweet_time.tz_localize('UTC')
            #--

            # fix time-format bug 2--

            start_time = pd.to_datetime(start_time)
            if start_time.tzinfo is None:
                start_time = start_time.tz_localize('UTC')

            end_time = pd.to_datetime(end_time)
            if end_time.tzinfo is None:
                end_time = end_time.tz_localize('UTC')
            #--

           
            coin_df['timestamp'] = pd.to_datetime(coin_df['timestamp'])
            resampled_df = coin_df.set_index('timestamp').resample(f'{minutes}min').agg({
                'close': 'last',  # Closing price for the time interval
                'high': 'max',    # Highest price
                'low': 'min',     # Lowest price
                'open': 'first'   # Opening price
            }).dropna().reset_index()
        
            # Filter data for the given time range
            filtered_df = resampled_df[
                (resampled_df['timestamp'] >= start_time) & 
                (resampled_df['timestamp'] <= end_time)
            ]
        
            # Find the closest timestamp to tweet_time
            if not filtered_df.empty:
                filtered_df['time_diff'] = abs(filtered_df['timestamp'] - tweet_time)
                closest_row_idx = filtered_df['time_diff'].idxmin()
            else:
                closest_row_idx = None
        
            # Convert the filtered data into the required dictionary format
            price_dict = []
            for idx, resampled_row in filtered_df.iterrows():
                price_dict.append({
                    "timestamp": int(resampled_row['timestamp'].timestamp() * 1000),  # Convert to ms
                    "oi": resampled_row['close'],  # Replace 'close' with 'oi' (or other relevant value)
                    "tweet_time": idx == closest_row_idx  # True only for the closest timestamp
                })
        
            # Append the result to the corresponding interval list
            price_record_tracks[interval].append(price_dict)
            
    # Add the new lists to the DataFrame
    for interval in time_intervals:
        detailed_mdf[f'price_{interval}'] = price_record_tracks[interval]
    
    # Add OI_data using generate_open_interest_data
    detailed_mdf['OI_data'] = detailed_mdf.apply(
        lambda row: generate_open_interest_data(row['coin'], row['tweet_date']),
        axis=1
    )
    
    # Write JSON with screen_name as the key
    def write_json_with_screen_name_key(detailed_mdf):
        # Save the flat JSON
        detailed_mdf.to_json('flat_match_history.json', orient='records', indent=4, lines=False)
        
        # Load the flat JSON
        with open('flat_match_history.json', 'r') as flat_json_file:
            flat_data = json.load(flat_json_file)
        
        # Group the data by 'screen_name'
        grouped_data = {}
        for record in flat_data:
            screen_name = record['screen_name']
            if screen_name not in grouped_data:
                grouped_data[screen_name] = []
            grouped_data[screen_name].append(record)
        
        # Save the reorganized JSON
        with open('match_history.json', 'w') as grouped_json_file:
            json.dump(grouped_data, grouped_json_file, indent=4)
    
    write_json_with_screen_name_key(detailed_mdf)
    
    return mdf,detailed_mdf


def get_open_interest_data():
    mongodb_url=''
    client = MongoClient(mongodb_url)
    db = client.graphscope
    
    collection = db.open_interest_crawl
    
    # Retrieve all records from the collection
    all_records = collection.find()
    
    # Retrieve all records from the collection and convert them to a list
    all_records = list(collection.find())
    
    
    # Convert the list of records into a pandas DataFrame
    df = pd.DataFrame(all_records)

    # Create an empty dictionary to store the result
    pair_dict = {}
    
    # Iterate over each row in the DataFrame
    for _, row in df.iterrows():
        date = row['date']
        data_list = row['data']  # The list of dicts in the 'data' column
    
        for item in data_list:
            pair = item['Pair']
            oi_vol = item['Open Interest']
            
    
            # Add the Pair to the dictionary if not already there
            if pair not in pair_dict:
                pair_dict[pair] = []
    
            # Append the date and OI/24h_vol as a tuple to the list for this Pair
            pair_dict[pair].append({"date": date, "Open Interest": oi_vol})
    
    # Convert the lists in the dictionary to DataFrames
    for pair in pair_dict:
        pair_dict[pair] = pd.DataFrame(pair_dict[pair])
        
        
        
    return pair_dict
    

def generate_open_interest_data(coin, target_date, day_set=7):
    def convert_to_numeric(value):
        if 'B' in value:
            return float(value.replace('$', '').replace('B', '')) * 1e9
        elif 'M' in value:
            return float(value.replace('$', '').replace('M', '')) * 1e6
        return None

    pair_dict = get_open_interest_data()
    
    # Ensure the target_date is in datetime format
    start_date = target_date - timedelta(days=day_set)
    end_date = target_date + timedelta(days=day_set)
    
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    target_date = pd.to_datetime(target_date)  # Ensure target_date is datetime

    if coin not in pair_dict:
     return []

    # Get the DataFrame for the coin
    coin_df = pair_dict[coin]
    
    # Ensure the date column is in datetime format
    coin_df["date"] = pd.to_datetime(coin_df["date"])
    
    # Filter the DataFrame for the date range
    filtered_df = coin_df[(coin_df["date"] >= start_date) & (coin_df["date"] <= end_date)]
    
    # Convert the Open Interest column to numeric
    filtered_df["Open Interest"] = filtered_df["Open Interest"].apply(convert_to_numeric)
    filtered_df=filtered_df.drop_duplicates('date',keep='last')
    
    # Create a list of dictionaries in the required format
    result = [
        {
            "timestamp": int(date.timestamp() * 1000),  # Convert date to Unix timestamp in milliseconds
            "oi": vol,  # Use the open interest value
            "tweet_time": date.date() == target_date.date()  # True if matches target_date, else False
        }
        for date, vol in zip(filtered_df["date"], filtered_df["Open Interest"])
    ]
    
    return result



        
        
def add_parameter(match_df,final_table):
    # Load the data
    
    data = match_df
    btc_df=pd.read_csv('coin_data/BTC.csv')
    
    
    result_dict={'screen_name':[],
          'SR':[],
          #'mar':[],
          #'alpha':[],
          'max_DD':[],
          'profit':[],
          }
    
    for kol, group in data.groupby('screen_name'):
        balance = 100
        price_each_trade=10
    
        broker=balance
        
        print('group',group)
    
        # make sure format is right
        group['check_time'] = pd.to_datetime(group['check_time'], infer_datetime_format=True, errors='coerce')
        group['tweet_date'] = pd.to_datetime(group['tweet_date'])
    
    
    
        # extracting
        open_order=group[['tweet_date','tweet_time']]
        open_order['time']=open_order['tweet_date']
        open_order['open_amount']=price_each_trade        
            
    
        close_order=group[['check_time']]
        close_order['time']=close_order['check_time']
        close_order['close_receive']=price_each_trade*(1+group[['ROI']]/100)
        
        trade_book=open_order.merge(close_order,on='time',how='outer')
        trade_book.fillna(0, inplace=True)
        trade_book=trade_book.drop('check_time',axis=1)
        trade_book['time'] = pd.to_datetime(trade_book['time'], errors='coerce')
        trade_book=trade_book.sort_values('time',ascending=True)
    
        asset_check=[]
        broker_check=[]
        date_list=[]
        for date, orders_in_day in trade_book.groupby('time'):
            date_list.append(date)
            #print('date',date)
            #print('orders_in_day',orders_in_day)
            orders_in_day['tweet_time'] = pd.to_datetime(orders_in_day['tweet_time'], errors='coerce')

            orders_in_day=orders_in_day.sort_values('tweet_time')
            # in one day
            for index, order in orders_in_day.iterrows():
    
                # check for closing an order (at the start of the day)
                close_receive=order['close_receive']
                #
                if close_receive > 0:
                    balance=balance-price_each_trade+close_receive
                    broker=broker+close_receive
                
    
                # check for opening an order (at start of the day)
                open_amount=order['open_amount']
                if broker>=open_amount:
                    broker=broker-open_amount
                    
    
                    
                #print(open_amount)
                asset_check.append(balance)
                broker_check.append(broker)
        
        trade_book['asset']=asset_check
        trade_book['broker']=broker_check
    
        asset_df=trade_book[['time','asset']]
        
        # calculate these parameter -----------------------------------------------
        
        # Ensure the date columns are in datetime format and sorted
        asset_df['time'] = pd.to_datetime(asset_df['time'])
        btc_df['date'] = pd.to_datetime(btc_df['date'])
        
        asset_df.sort_values('time', inplace=True)
        btc_df.sort_values('date', inplace=True)
        
        # Calculate daily portfolio returns
        asset_df['returns'] = asset_df['asset'].pct_change()
        
        # Calculate BTC benchmark daily returns
        btc_df['returns'] = btc_df['close'].pct_change()
        
        # Merge asset_df and btc_df on the closest date to align data
        
        # Ensure datetime columns are of the same type
        asset_df['time'] = pd.to_datetime(asset_df['time']).dt.tz_localize(None)  # Convert to timezone-naive
        btc_df['date'] = pd.to_datetime(btc_df['date']).dt.tz_localize(None)  # Convert to timezone-naive
        
        # Perform the merge_asof operation
        merged_df = pd.merge_asof(
            asset_df, 
            btc_df, 
            left_on='time', 
            right_on='date', 
            direction='backward', 
            suffixes=('_portfolio', '_btc')
        )    
        
        # Risk-free rate (use a fixed value, e.g., 0.03 for 3% annualized rate)
        risk_free_rate = 0.03 / 252  # Convert to daily rate
        
        # 1. Calculate MAR Ratio
        # Calculate CAGR
        start_value = asset_df['asset'].iloc[0]
        end_value = asset_df['asset'].iloc[-1]
        num_years = (asset_df['time'].iloc[-1] - asset_df['time'].iloc[0]).days / 365.25
        cagr = (end_value / start_value) ** (1 / num_years) - 1
        
        # Calculate Maximum Drawdown
        cumulative = (1 + asset_df['returns']).cumprod()
        rolling_max = cumulative.cummax()
        drawdown = (cumulative - rolling_max) / rolling_max
        max_drawdown = drawdown.min()
        
        #mar_ratio = cagr / abs(max_drawdown)
        
        # 2. Calculate Alpha
        # Calculate beta (correlation * std_dev_portfolio / std_dev_benchmark)
        
        # Drop NaN and align indices
        returns_portfolio = merged_df['returns_portfolio'].dropna()
        returns_btc = merged_df['returns_btc'].dropna()
        
        # Align the indices
        aligned_data = pd.concat([returns_portfolio, returns_btc], axis=1, join='inner')
        aligned_portfolio = aligned_data['returns_portfolio']
        aligned_btc = aligned_data['returns_btc']
        
        # Calculate the covariance matrix
        cov_matrix = np.cov(aligned_portfolio, aligned_btc)    
            
        beta = cov_matrix[0, 1] / cov_matrix[1, 1]
        
        # Calculate Alpha
        portfolio_excess_return = merged_df['returns_portfolio'].mean() - risk_free_rate
        benchmark_excess_return = merged_df['returns_btc'].mean() - risk_free_rate
        #alpha = portfolio_excess_return - beta * benchmark_excess_return
        
        # 3. Calculate Sharpe Ratio
        sharpe_ratio = portfolio_excess_return / merged_df['returns_portfolio'].std()
        
        # 4. Calculate Maximum Drawdown (already calculated for MAR)
        max_drawdown_value = max_drawdown*100
        
        result_dict['screen_name'].append(kol)
        result_dict['SR'].append(sharpe_ratio)
        #result_dict['mar'].append(mar_ratio)
        #result_dict['alpha'].append(alpha)
        result_dict['max_DD'].append(max_drawdown_value)
        result_dict['profit'].append(balance-100)
    
        if kol == 'crypto_rand':
            # Customize the plot
            plt.plot(trade_book["time"], trade_book["asset"], label=f"Assets ")
            plt.plot(trade_book["time"], trade_book["broker"], label=f"Broker ")
            plt.title("Asset Trends by Broker Over Time", fontsize=16)
            plt.xlabel("Time", fontsize=12)
            plt.ylabel("Assets", fontsize=12)
            plt.legend(title="Broker")
            plt.xticks(rotation=45)  # Rotate x-axis labels 45 degrees
            plt.grid(True)
            
            # Display the plot
            plt.show()
    
    
        #break
    
    result_df=pd.DataFrame(result_dict)
        
    
    final_rank_df=final_table.merge(result_df,on='screen_name',how='left')
    return final_rank_df


def kol_data_to_json(mdf,final_table):
    
    mdf.index = pd.to_datetime(mdf.index)
    
    # Define the date range for the last 90 days and get today's date
    last_90_days = mdf.index.max() - pd.Timedelta(days=90)
    today = pd.Timestamp.now()
    
    
    def get_elo_after_dict(group):
        group['tweet_date'] = pd.to_datetime(group['tweet_date'])

        # Filter for elo_after values in the last 90 days using tweet dates instead of the index
        recent_data = group[group['tweet_date'] >= last_90_days][['tweet_date', 'elo_after']]
        
        # Convert tweet dates and elo values into a dictionary
        elo_dict = dict(zip(recent_data['tweet_date'], recent_data['elo_after']))
        
        # If recent_data is empty, add two dates with the last known elo value
        if not elo_dict:
            last_value = group['elo_after'].iloc[0]
            elo_dict = {last_90_days: last_value, today: last_value}
        else:
            # Add an entry for today with the last known elo value within the 90-day range
            last_value = recent_data['elo_after'].iloc[-1]
            elo_dict[today] = last_value
       
        elo_dict = {Timestamp(date): value for date, value in elo_dict.items()}       
        elo_dict = {int(date.timestamp() * 1000): value for date, value in elo_dict.items()}
        #print('elo_dict',elo_dict)
        
        return elo_dict
        
    # Define coin distribution function (from previous example)
    def calculate_coin_distribution(group):
        coin_counts = group['coin'].value_counts(normalize=True)
        coin_distribution = {}
        for coin, pct in coin_counts.items():
            if pct >= 0.10:
                coin_distribution[coin] = pct
            else:
                coin_distribution['Other'] = coin_distribution.get('Other', 0) + pct
        return coin_distribution
    
    # Group by 'screen_name' and calculate each column as specified
    result_mdf = mdf[mdf['ghost_match']==False].groupby('screen_name').apply(lambda group: pd.Series({
        'elo_recent_90_days': get_elo_after_dict(group),
        'coin_distribution': calculate_coin_distribution(group),
        'average_elo_recent_10_opinions': group.sort_index(ascending=False).head(10)['elo_after'].mean(),
        'win_rate': (group['game_result'] == 'w').mean(),
        'win_match': (group['game_result'] == 'w').sum(),  # Count of wins
        'lose_match': (group['game_result'] == 'l').sum(),  # Count of losses
        'win_rate_recent_10_matches': (group.sort_index(ascending=False).head(10)['game_result'] == 'w').mean(),
        'highest_elo_rate_ever': group['elo_after'].max(),  # Calculate the highest elo rate ever

    })).reset_index()
    
    #  Adding tier 
    
    result_mdf=result_mdf.merge(final_table[['screen_name','name','tier','rank','SR','max_DD','profit','average_ROI']],on='screen_name',how='left')
    
    
    result = {}
    for screen_name, group in result_mdf.groupby('screen_name'):
        # Convert each group's DataFrame to a list of dictionaries
        records = group.to_dict(orient='records')
          
        # Update each record to replace 'screen_name' with 'profile_link' containing the Twitter link
        for record in records:
            # Convert the 'elo_recent_90_days' keys from Timestamp to string
            record['elo_recent_90_days'] = {str(k): v for k, v in record['elo_recent_90_days'].items()}
            
            # Convert screen_name to profile link
            record['profile_link'] = f"https://x.com/{screen_name}"
            del record['screen_name']  # Remove the original 'screen_name' field
        
        # Add the modified records to the result dictionary with the original screen_name as the key
        result[screen_name] = records
    
    
    
    # Save the result to a JSON file
    with open('kol_data.json', 'w') as file:
        json.dump(result, file, indent=4)
        